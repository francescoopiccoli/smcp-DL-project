{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caYTh8Bf9K_L"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVWav8HpqmSd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft2ARjvm9OQx"
      },
      "source": [
        "Change directory to myDrive (smcp must be in myDrive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ETLDLzBqpNr"
      },
      "outputs": [],
      "source": [
        "%cd 'gdrive/MyDrive'\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2wRgx_L9gCz"
      },
      "source": [
        "Uninstall problem causing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH-ElDK0qrHi"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torchtext torchaudio torchdata tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLmuMQDV975U"
      },
      "source": [
        "Install correct versions of packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyd-EpRXykfK"
      },
      "outputs": [],
      "source": [
        "!pip install lightning-bolts==0.5.0 tabulate torch==1.11.0 torchvision==0.12.0 torchmetrics==0.9.1 pytorch-lightning==1.5.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-fl-r11qtYm"
      },
      "outputs": [],
      "source": [
        "# !pip install tqdm==4.65.0 tensorboard==2.11.2 torch==1.10.0 lightning-lite==1.8.0 pytorch-lightning==1.8.0 torchmetrics==0.11.4 torchvision==0.11.1 lightning-utilities==0.3.0 lightning-bolts==0.6.0.post1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRjnorFRofT2"
      },
      "outputs": [],
      "source": [
        "#!pip install pytorch-lightning==1.4.9 torch==1.8.0 torchmetrics==0.7.0 torchtext==0.9.0 torchvision==0.9.0 lightning-bolts==0.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbcpI0hArq_5"
      },
      "source": [
        "New data module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXKIfCcxrqxN"
      },
      "outputs": [],
      "source": [
        "import torchvision as tv\n",
        "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
        "from pl_bolts.datamodules import CIFAR10DataModule\n",
        "\n",
        "def get_default(data_dir, batch_size, num_workers):\n",
        "    train_transforms = tv.transforms.Compose([\n",
        "        tv.transforms.RandomCrop(32, padding=4),\n",
        "        tv.transforms.RandomHorizontalFlip(),\n",
        "        tv.transforms.ToTensor(),\n",
        "        cifar10_normalization()\n",
        "    ])\n",
        "\n",
        "    test_transforms = tv.transforms.Compose([\n",
        "        tv.transforms.ToTensor(),\n",
        "        cifar10_normalization()\n",
        "    ])\n",
        "\n",
        "    cifar10_dm = CIFAR10DataModule(\n",
        "        data_dir=data_dir,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        train_transforms=train_transforms,\n",
        "        test_transforms=test_transforms,\n",
        "        val_transforms=test_transforms\n",
        "\n",
        "    )\n",
        "    return cifar10_dm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vhQTBej-Yqa"
      },
      "source": [
        "Copy of image_classifier.py (with small modifications for running in interactive env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiy-c6bDqvGK"
      },
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "from typing import Dict, Tuple, Union\n",
        "import warnings\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning.plugins import DDPPlugin\n",
        "import pytorch_lightning.callbacks as pt_callbacks\n",
        "from pytorch_lightning.utilities.warnings import LightningDeprecationWarning\n",
        "import torch\n",
        "from torch.distributed.algorithms.ddp_comm_hooks.default_hooks import fp16_compress_hook\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchmetrics import MetricCollection, Accuracy\n",
        "\n",
        "from smcp.core.enum_parse import EnumAction\n",
        "from smcp.core.lr_scheduler import WarmupMultiStepLR, WarmupCosineLR, WarmupLinearLR, WarmupExponentialLR\n",
        "from sparse_ops import create_coster, create_importance_accumulator, ChannelBNRescalingType, ChannelPruning, ChannelPruningSchedule, ChannelPruningType, CostingType, DynamicPruning, ImportanceAccumulatorType, ImportanceType, ImportanceGradType, ImportanceHessType, ParameterMaskingType, PruningLogVerbosity, PruningSchedule\n",
        "from smcp.classification.datasets import UpscaledCIFAR10DataModule, UpscaledCIFAR100DataModule, ImagenetDataModule\n",
        "from smcp.classification.models import get_classification_model\n",
        "from smcp.classification.losses import LabelSmoothing\n",
        "\n",
        "# Disable pl deprecations\n",
        "warnings.simplefilter(\"ignore\", LightningDeprecationWarning)\n",
        "\n",
        "class ImageClassifierParams:\n",
        "    arch: str\n",
        "    pretrained: Union[bool, str]\n",
        "    num_classes: int\n",
        "    label_smoothing: float\n",
        "    learning_rate: float\n",
        "    momentum: float\n",
        "    nesterov: bool\n",
        "    weight_decay: float\n",
        "    bn_weight_decay: float\n",
        "    lr_schedule: str\n",
        "    warmup: int\n",
        "    epochs: int\n",
        "\n",
        "class ImageClassifier(pl.LightningModule):\n",
        "    hparams: ImageClassifierParams\n",
        "    model: nn.Module\n",
        "\n",
        "    def __init__(\n",
        "        self, arch: str, num_classes: int, label_smoothing: float, pretrained: Union[bool, str],\n",
        "        learning_rate: float, momentum: float, nesterov: bool, weight_decay: float, bn_weight_decay: float,\n",
        "        lr_schedule: str, warmup: int, epochs: int, **kwargs\n",
        "    ):\n",
        "        \"\"\"Image Classifier model\n",
        "        Args:\n",
        "            arch: type of classifier architecture\n",
        "            num_classes: number of image classes\n",
        "            label_smoothing: [0, 1) value for label smoothing\n",
        "            pretrained: whether to use a pretrained network. If a string, path to the pretrained weights\n",
        "            learning_rate: learning rate\n",
        "            momentum: SGD momentum\n",
        "            nesterov: whether to enable Nesterov momentum\n",
        "            weight_decay: amount of weight decay for non-BatchNorm weights\n",
        "            bn_weight_decay: amount of weight decay for BatchNorm weights\n",
        "            lr_schedule: LR scheduler type\n",
        "            warmup: LR scheduler linear warmup time\n",
        "            epochs: total number of training epochs\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.model = get_classification_model(arch, num_classes, pretrained=pretrained).to(memory_format=torch.channels_last)\n",
        "        self.example_input_array = torch.ones(1, 3, 224, 224).to(memory_format=torch.channels_last)\n",
        "\n",
        "\n",
        "        if label_smoothing > 0.0:\n",
        "            self.criterion = LabelSmoothing(label_smoothing)\n",
        "        else:\n",
        "            self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        acc_metrics = MetricCollection({\n",
        "            \"top1\": Accuracy(num_classes=num_classes, top_k=1),\n",
        "            \"top5\": Accuracy(num_classes=num_classes, top_k=5)\n",
        "        })\n",
        "        self.train_acc_metrics = acc_metrics.clone(prefix=\"train/\")\n",
        "        self.val_acc_metrics = acc_metrics.clone(prefix=\"val/\")\n",
        "\n",
        "    def configure_optimizers(self) -> optim.Optimizer:\n",
        "        parameters_for_optimizer = list(self.model.named_parameters())\n",
        "\n",
        "        bn_params = [v for n, v in parameters_for_optimizer if \"bn\" in n]\n",
        "        rest_params = [v for n, v in parameters_for_optimizer if not \"bn\" in n]\n",
        "        optimizer = optim.SGD(\n",
        "            [\n",
        "                {\"params\": bn_params, \"weight_decay\": self.hparams.bn_weight_decay},\n",
        "                {\"params\": rest_params, \"weight_decay\": self.hparams.weight_decay}\n",
        "            ],\n",
        "            self.hparams.learning_rate,\n",
        "            momentum=self.hparams.momentum,\n",
        "            weight_decay=self.hparams.weight_decay,\n",
        "            nesterov=self.hparams.nesterov\n",
        "        )\n",
        "\n",
        "        lr_scheduler = None\n",
        "        if self.hparams.lr_schedule == \"step\":\n",
        "            lr_scheduler = WarmupMultiStepLR(optimizer, self.hparams.warmup, [30,60,80], 0.1)\n",
        "        elif self.hparams.lr_schedule == \"step_prune\":\n",
        "            lr_scheduler = WarmupMultiStepLR(optimizer, self.hparams.warmup, [10,20,30], 0.1)\n",
        "        elif self.hparams.lr_schedule == \"cosine\":\n",
        "            lr_scheduler = WarmupCosineLR(optimizer, self.hparams.warmup, self.hparams.epochs)\n",
        "        elif self.hparams.lr_schedule == \"linear\":\n",
        "            lr_scheduler = WarmupLinearLR(optimizer, self.hparams.warmup, self.hparams.epochs)\n",
        "        elif self.hparams.lr_schedule == \"exponential\":\n",
        "            lr_scheduler = WarmupExponentialLR(optimizer, self.hparams.warmup, gamma=0.98) #ours\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": lr_scheduler\n",
        "        }\n",
        "\n",
        "    def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: optim.Optimizer, optimizer_idx: int) -> None:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "        x, target = batch\n",
        "\n",
        "        logits = self.forward(x)\n",
        "        loss = self.criterion(logits, target)\n",
        "\n",
        "        self.log(\"train/loss\", loss, sync_dist=True)\n",
        "\n",
        "        preds = nn.functional.softmax(logits, dim=1)\n",
        "        acc_metrics = self.train_acc_metrics(preds, target)\n",
        "        self.log_dict(acc_metrics, sync_dist=True)\n",
        "\n",
        "        return { \"loss\": loss }\n",
        "\n",
        "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> Dict[str, torch.Tensor]:\n",
        "        x, target = batch\n",
        "\n",
        "        logits = self.forward(x)\n",
        "        loss = self.criterion(logits, target)\n",
        "\n",
        "        self.log(\"val/loss\", loss, sync_dist=True)\n",
        "\n",
        "        preds = nn.functional.softmax(logits, dim=1)\n",
        "        acc_metrics = self.val_acc_metrics(preds, target)\n",
        "        self.log_dict(acc_metrics, sync_dist=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def main(hparams):\n",
        "    # Interpret/modify the hparams\n",
        "    using_gpu = hparams.gpus is not None\n",
        "    data_dtype = torch.float16 if hparams.fp16 else torch.float32\n",
        "    precision = 16 if hparams.fp16 else 32\n",
        "    accum_grad_batches = hparams.simulated_batch_size // hparams.batch_size\n",
        "    accelerator = \"ddp\" if using_gpu else None\n",
        "    sync_batchnorm =  using_gpu and hparams.batch_size <= 32\n",
        "\n",
        "    eff_batch_size = hparams.batch_size * accum_grad_batches * hparams.num_nodes * (hparams.gpus if using_gpu else 1)\n",
        "    hparams.learning_rate *= eff_batch_size / 256\n",
        "    hparams.rewiring_freq = ceil(256 * hparams.rewiring_freq / eff_batch_size)\n",
        "\n",
        "    # Setup datamodule\n",
        "    dm_cls = None\n",
        "    if hparams.dataset == \"Imagenet\":\n",
        "        dm_cls = ImagenetDataModule\n",
        "    elif hparams.dataset == \"CIFAR10\":\n",
        "        dm_cls = UpscaledCIFAR10DataModule\n",
        "    elif hparams.dataset == \"CIFAR100\":\n",
        "        dm_cls = UpscaledCIFAR100DataModule\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Dataset {hparams.dataset} unknown\")\n",
        "\n",
        "    # dm = dm_cls(\n",
        "    #     hparams.data_root, num_workers=hparams.workers, batch_size=hparams.batch_size,\n",
        "    #     shuffle=True, pin_memory=using_gpu, drop_last=True, dtype=data_dtype\n",
        "    # )\n",
        "    dm = get_default(hparams.data_root, batch_size=hparams.batch_size, num_workers=hparams.workers)\n",
        "\n",
        "    # Setup model\n",
        "    model = ImageClassifier(num_classes=dm.num_classes, **vars(hparams))\n",
        "\n",
        "    # Setup trainer\n",
        "    pl.seed_everything(hparams.seed, workers=True)\n",
        "\n",
        "    logger = pl.loggers.TensorBoardLogger(\n",
        "        save_dir=hparams.output_dir,\n",
        "        name=f\"image_classifier-{hparams.dataset}\",\n",
        "        default_hp_metric=False\n",
        "    )\n",
        "    callbacks = [\n",
        "        pl.callbacks.LearningRateMonitor(logging_interval=\"epoch\"),\n",
        "        # pl.callbacks.ModelCheckpoint(\n",
        "        #     filename=\"image_classifier-epoch{epoch}-val_loss{val/loss:.4f}-top1{val/top1:.4f}\",\n",
        "        #     mode=\"max\",\n",
        "        #     monitor=\"val/top1\",\n",
        "        #     auto_insert_metric_name=False,\n",
        "        #     save_last=True,\n",
        "        #     every_n_val_epochs=hparams.ckpt_freq\n",
        "        # )\n",
        "    ]\n",
        "\n",
        "    if hparams.prune:\n",
        "        importance_accum = create_importance_accumulator(hparams.importance_accumulator)\n",
        "\n",
        "        if hparams.channel_type is not ChannelPruningType.Skip:\n",
        "            coster = create_coster(hparams.costing_type, hparams.costing_latency_table)\n",
        "\n",
        "            pruning_schedule = ChannelPruningSchedule(\n",
        "                hparams.channel_ratio, hparams.channel_schedule,\n",
        "                hparams.epochs, hparams.prune_warmup, hparams.channel_schedule_length, hparams.prune_cooldown, hparams.rewiring_freq\n",
        "            )\n",
        "\n",
        "            unpruned_layers = [\"model.conv1\", \"model.conv_bn\"]\n",
        "            pruning_method = ChannelPruning(\n",
        "                hparams.masking_type, pruning_schedule, importance_accum, coster,\n",
        "                hparams.channel_type, unpruned_layers, hparams.channel_chunk_size, hparams.channel_allow_layer_prune, hparams.channel_bnrescaling_type,\n",
        "                hparams.channel_doublesided_weight, track_mask_convergence=True\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(\"Pruning is set but an unrecognized configuration was given\")\n",
        "\n",
        "        pruning_callback = DynamicPruning(\n",
        "            pruning_method, hparams.importance_type, hparams.importance_grad_type,\n",
        "            hparams.importance_hess_type, hparams.pruned_decay, True,\n",
        "            log_verbosity=PruningLogVerbosity.Full\n",
        "        )\n",
        "        callbacks.append(pruning_callback)\n",
        "\n",
        "    plugins = []\n",
        "    if accelerator == \"ddp\":\n",
        "        plugins.append(DDPPlugin(\n",
        "            find_unused_parameters=False,\n",
        "            gradient_as_bucket_view=True,\n",
        "            ddp_comm_hook=fp16_compress_hook if hparams.fp16 else None\n",
        "        ))\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        accelerator=accelerator, num_nodes=hparams.num_nodes, gpus=hparams.gpus,\n",
        "        benchmark=using_gpu, sync_batchnorm=sync_batchnorm,\n",
        "        max_epochs=hparams.epochs, precision=precision, accumulate_grad_batches=accum_grad_batches,\n",
        "        gradient_clip_val=hparams.clip, log_every_n_steps=hparams.train_log_freq,\n",
        "        plugins=plugins, callbacks=callbacks, logger=logger, weights_summary=\"full\", enable_checkpointing=False\n",
        "    )\n",
        "\n",
        "    # Run experiment\n",
        "    trainer.fit(model, datamodule=dm)\n",
        "\n",
        "    # Perform final validation\n",
        "    trainer.validate(model, datamodule=dm)\n",
        "\n",
        "    # Save the model (without training state)\n",
        "    torch.save(model.model.state_dict(), f\"{logger.log_dir}/80_pruning.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7lPJ7zU-ksl"
      },
      "source": [
        "Train classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7qdK5xUq3kn"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "    self.arch = \"resnet18\"\n",
        "    self.pretrained = False\n",
        "    self.dataset = \"CIFAR10\" #Imagenet\n",
        "    self.data_root = \".\"\n",
        "    self.workers = 0 # 8\n",
        "    self.batch_size = 64 # 256\n",
        "    self.simulated_batch_size = 64 # -1\n",
        "    self.learning_rate = 0.1 #0.256\n",
        "    self.momentum = 0.875\n",
        "    self.nesterov = False\n",
        "    self.weight_decay = 3.0517578125e-05\n",
        "    self.bn_weight_decay = 0.0\n",
        "    self.clip = None # 0.01\n",
        "    self.epochs = 21 # 90\n",
        "    self.lr_schedule = \"exponential\" #choices=[\"step\", \"linear\", \"cosine\", \"step_prune\"])\n",
        "    self.warmup = 0\n",
        "    self.label_smoothing = 0.1\n",
        "    self.seed = 42\n",
        "    self.num_nodes = 1\n",
        "    self.gpus = 1\n",
        "    self.fp16 = False\n",
        "    self.output_dir = \"logs/\"\n",
        "    self.train_log_freq = 50\n",
        "    self.ckpt_freq = 5\n",
        "    self.prune = True #False\n",
        "    self.masking_type = ParameterMaskingType.Soft\n",
        "    self.prune_warmup = 0\n",
        "    self.prune_cooldown = 5\n",
        "    self.rewiring_freq = 10\n",
        "    self.pruned_decay = 2e-4\n",
        "    self.channel_type = ChannelPruningType.Global\n",
        "    self.channel_ratio = 0.8 # 0\n",
        "    self.channel_chunk_size = 1\n",
        "    self.channel_allow_layer_prune = False\n",
        "    self.channel_schedule = \"exp\"\n",
        "    self.channel_schedule_length = 16 # 55\n",
        "    self.channel_bnrescaling_type = ChannelBNRescalingType.Skip\n",
        "    self.channel_doublesided_weight = 1\n",
        "    self.importance_type = ImportanceType.Weight\n",
        "    self.importance_grad_type = ImportanceGradType.INST\n",
        "    self.importance_hess_type = ImportanceHessType.GRADSQ\n",
        "    self.importance_accumulator = ImportanceAccumulatorType.Latest\n",
        "    self.costing_type = CostingType.Flop\n",
        "    self.costing_latency_table = \".\" # \"./latency_tables/resnet50_titanV_cudnn74.pkl\"\n",
        "\n",
        "args = Args()\n",
        "main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj05MBP8-oLO"
      },
      "source": [
        "Calculate time (FPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP4j0jaOvmuy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "def time_inference(\n",
        "    model: nn.Module, dataloader: DataLoader, num_batches: int = 30, warmup: int = 10\n",
        ") -> float:\n",
        "    cudnn.benchmark = True\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "    start_evt = torch.cuda.Event(enable_timing=True)\n",
        "    end_evt = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    times = []\n",
        "    for i, (input, target) in enumerate(dataloader):\n",
        "        input = input.to(device)\n",
        "\n",
        "        start_evt.record()\n",
        "        output = model(input)\n",
        "        end_evt.record()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed_time = start_evt.elapsed_time(end_evt)\n",
        "\n",
        "        if i < warmup:\n",
        "            continue\n",
        "\n",
        "        times.append(elapsed_time)\n",
        "\n",
        "        if i >= warmup + num_batches:\n",
        "            break\n",
        "    fps = sum(times) / len(times)\n",
        "    print(\"FPS:\", fps)\n",
        "    return fps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6UZko6VnV8F"
      },
      "outputs": [],
      "source": [
        "dm = get_default(\".\", batch_size=64, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk4gXXjxSuBq"
      },
      "outputs": [],
      "source": [
        "dm.prepare_data()\n",
        "dm.setup()\n",
        "dl = dm.val_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FeicDaodgpB"
      },
      "outputs": [],
      "source": [
        "m = get_classification_model('resnet18', 10, pretrained=False).to(memory_format=torch.channels_last)\n",
        "path = \"logs/image_classifier-CIFAR10/version_45/70_pruning.pt\"\n",
        "m.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu8YO-R_dllG"
      },
      "outputs": [],
      "source": [
        "fps = time_inference(m, dl)\n",
        "print(\"FPS:\", fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXr8q3d6UG3M"
      },
      "outputs": [],
      "source": [
        "# no_pruning.pt top1 = 0.7883999943733215  top5 = 0.9855999946594238 FPS = 8.281613411441926 version_38\n",
        "# 10_pruning.pt top1 = 0.7860999703407288 top5 = 0.9861000180244446 FPS = 8.389946783742596 version_39\n",
        "# 20_pruning.pt top1 = 0.7764999866485596 top5 = 0.9865999817848206 FPS = 8.39308895603303 version_40\n",
        "# 30_pruning.pt top1 = 0.7702216204049691 top5 = 0.9853000044822693 FPS = 9.072773195082142 version_41\n",
        "# 40_pruning.pt top1 = 0.7700999975204468 top5 = 0.986299991607666 FPS = 8.629408990183185 version_42\n",
        "# 50_pruning.pt top1 = 0.7663999795913696 top5 = 0.9850000143051147 FPS = 8.281490695091986 version_43\n",
        "# 60_pruning.pt top1 = 0.7562000155448914 top5 = 0.984000027179718 FPS = 9.224685422835812 version_44\n",
        "# 70_pruning.pt top1 = 0.6859999895095825 top5 = 0.9761000275611877 FPS =  9.628498600375268 version 45\n",
        "# 80_pruning.pt top1 =  top5 =  FPS =  \n",
        "# 90_pruning.pt top1 =  top5 =  FPS =  \n",
        "\n",
        "# 32_bs.pt\n",
        "# 64_bs.pt\n",
        "# 128_bs.pt\n",
        "# 256_bs.pt\n",
        "\n",
        "# 10_rewiring_freq.pt\n",
        "# 20_rewiring_freq.pt\n",
        "# 30_rewiring_freq.pt\n",
        "\n",
        "# 256_lr.pt\n",
        "# 512_lr.pt\n",
        "# 768_lr.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3y379boQrhv"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Lkk6rRQ46H"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jawoOXH2Q_5G"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwn7QtnFMRtQ"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/image_classifier-CIFAR10/version_38/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the excel file with the results for the pruning ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0a7J7DZQoxg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('channel_ratio_results.xlsx')\n",
        "print(df)\n",
        "\n",
        "channel_ratio_values = df['Channel Ratio'].tolist()\n",
        "top_1_values_raw = df['top_1'].tolist()\n",
        "fps_values = df['FPS'].tolist()\n",
        "\n",
        "top_1_values = []\n",
        "for item in top_1_values_raw:\n",
        "    if isinstance(item, str):\n",
        "        item = item.replace('\\n', '').replace(',', '.')\n",
        "        item = float(item)\n",
        "    top_1_values.append(item)\n",
        "\n",
        "print(channel_ratio_values)\n",
        "print(top_1_values)\n",
        "print(fps_values)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the Top1 / FPS for different pruning ratios "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results_top1(channel_ratio_values, one_accs, fps):\n",
        "  \n",
        "  plt.plot(fps, one_accs)\n",
        "\n",
        "  plt.title(\"Top-1 Accuracy vs Frames per Second (FPS) for different pruning ratios\")\n",
        "  plt.xlabel(\"Frames per Second (FPS)\")\n",
        "  plt.ylabel(\"Top-1 Accuracy\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.scatter(fps, one_accs, s=100, c=\"r\", marker=\"o\")\n",
        "  \n",
        "  # add labels for specific points\n",
        "  for i in range(len(one_accs)):\n",
        "    plt.text(fps[i], one_accs[i], str(channel_ratio_values[i] * 100) + \"%\", fontsize=6)\n",
        "  \n",
        "\n",
        "plot_results_top1(channel_ratio_values, top_1_values, fps_values)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
